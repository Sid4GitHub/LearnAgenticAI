{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1796c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_tools import convert_mcp_to_langchain_tools\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.prebuilt import create_react_agent \n",
    "from langchain_mcp_tools import convert_mcp_to_langchain_tools\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "import subprocess "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46384fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function convert_mcp_to_langchain_tools in module langchain_mcp_tools.langchain_mcp_tools:\n",
      "\n",
      "async convert_mcp_to_langchain_tools(server_configs: dict[str, langchain_mcp_tools.langchain_mcp_tools.McpServerCommandBasedConfig | langchain_mcp_tools.langchain_mcp_tools.McpServerUrlBasedConfig], logger: logging.Logger | int | None = None) -> tuple[list[langchain_core.tools.base.BaseTool], typing.Callable[[], typing.Awaitable[NoneType]]]\n",
      "    Initialize multiple MCP servers and convert their tools to LangChain format.\n",
      "    \n",
      "    This is the main entry point for the library. It orchestrates the complete\n",
      "    lifecycle of multiple MCP server connections, from initialization through\n",
      "    tool conversion to cleanup. Provides robust error handling and authentication\n",
      "    pre-validation to prevent common MCP client library issues.\n",
      "    \n",
      "    Key Features:\n",
      "    - Parallel initialization of multiple servers for efficiency\n",
      "    - Authentication pre-validation for HTTP servers to prevent async generator bugs\n",
      "    - Automatic transport selection and fallback per MCP specification\n",
      "    - Comprehensive error handling with McpInitializationError\n",
      "    - User-controlled cleanup via returned async function\n",
      "    - Support for both local (stdio) and remote (HTTP/WebSocket) servers\n",
      "    \n",
      "    Transport Support:\n",
      "    - stdio: Local command-based servers (npx, uvx, python, etc.)\n",
      "    - streamable_http: Modern HTTP servers (recommended, tried first)\n",
      "    - sse: Legacy Server-Sent Events HTTP servers (fallback)\n",
      "    - websocket: WebSocket servers for real-time communication\n",
      "    \n",
      "    Error Handling:\n",
      "    All configuration and connection errors are wrapped in McpInitializationError\n",
      "    with server context for easy debugging. Authentication failures are detected\n",
      "    early to prevent async generator cleanup issues in the MCP client library.\n",
      "    \n",
      "    Args:\n",
      "        server_configs: Dictionary mapping server names to configurations.\n",
      "            Each config can be either McpServerCommandBasedConfig for local\n",
      "            servers or McpServerUrlBasedConfig for remote servers.\n",
      "        logger: Optional logger instance. If None, creates a pre-configured\n",
      "            logger with appropriate levels for MCP debugging.\n",
      "            If a logging level (e.g., `logging.DEBUG`), the pre-configured\n",
      "            logger will be initialized with that level.\n",
      "    \n",
      "    Returns:\n",
      "        A tuple containing:\n",
      "        - List[BaseTool]: All tools from all servers, ready for LangChain use\n",
      "        - McpServerCleanupFn: Async function to properly shutdown all connections\n",
      "    \n",
      "    Raises:\n",
      "        McpInitializationError: If any server fails to initialize with detailed context\n",
      "    \n",
      "    Example:\n",
      "        server_configs = {\n",
      "            \"local-filesystem\": {\n",
      "                \"command\": \"npx\",\n",
      "                \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", \".\"]\n",
      "            },\n",
      "            \"remote-api\": {\n",
      "                \"url\": \"https://api.example.com/mcp\",\n",
      "                \"headers\": {\"Authorization\": \"Bearer your-token\"},\n",
      "                \"timeout\": 30.0\n",
      "            }\n",
      "        }\n",
      "    \n",
      "        try:\n",
      "            tools, cleanup = await convert_mcp_to_langchain_tools(server_configs)\n",
      "            \n",
      "            # Use tools with your LangChain application\n",
      "            for tool in tools:\n",
      "                result = await tool.arun(**tool_args)\n",
      "                \n",
      "        except McpInitializationError as e:\n",
      "            print(f\"Failed to initialize MCP server '{e.server_name}': {e}\")\n",
      "            \n",
      "        finally:\n",
      "            # Always cleanup when done\n",
      "            await cleanup()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "help(convert_mcp_to_langchain_tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f488edc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    server_configs = {\n",
    "        \"add_tool\": {\n",
    "            \"command\": \"D:\\\\Work\\\\miniconda_env\\\\try-mcp-2.x\\\\python.exe\",\n",
    "            \"args\": [\"D:\\\\Work\\\\wsl\\\\Git\\\\LearnAgenticAI\\\\mcp\\\\example-3\\\\server\\\\add_server.py\"],\n",
    "            \"transport\": \"stdio\",\n",
    "            \"errlog\": subprocess.DEVNULL \n",
    "        },\n",
    "        \"multiply_tool\": {\n",
    "            \"url\": \"http://127.0.0.1:8000/mcp\",\n",
    "            \"transport\": \"streamable-http\",\n",
    "            \"errlog\": subprocess.DEVNULL \n",
    "        }\n",
    "    }\n",
    "\n",
    "    tools, cleanup = await convert_mcp_to_langchain_tools(server_configs)\n",
    "\n",
    "\n",
    "    print(\"Available tools:\", tools)\n",
    "\n",
    "    llm   = ChatOllama(model=\"llama3.2:3b\", temperature=0)\n",
    "    agent = create_react_agent(llm, tools)\n",
    "\n",
    "    # Example queries\n",
    "    for query in [\"What is 7 + 13?\", \"Compute 6 × 9.\"]:\n",
    "        result = await agent.ainvoke(\n",
    "            {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n",
    "        )\n",
    "        last   = result[\"messages\"][-1]\n",
    "        answer = last.get(\"content\") if isinstance(last, dict) else last.content\n",
    "        print(f\"Query: {query}\\nAnswer: {answer}\\n\")\n",
    "\n",
    "    # Clean up MCP server sessions\n",
    "    await cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68a9a4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[90m[INFO]\u001b[0m MCP server \"add_tool\": initializing with: {'command': 'D:\\\\Work\\\\miniconda_env\\\\try-mcp-2.x\\\\python.exe', 'args': ['D:\\\\Work\\\\wsl\\\\Git\\\\LearnAgenticAI\\\\mcp\\\\example-3\\\\server\\\\add_server.py'], 'transport': 'stdio', 'errlog': -3}\n",
      "\u001b[90m[INFO]\u001b[0m MCP server \"add_tool\": spawning local process via stdio\n",
      "\u001b[90m[INFO]\u001b[0m MCP server \"multiply_tool\": initializing with: {'url': 'http://127.0.0.1:8000/mcp', 'transport': 'streamable-http', 'errlog': -3}\n",
      "\u001b[90m[INFO]\u001b[0m MCP server \"multiply_tool\": Pre-validating authentication\n",
      "\u001b[90m[INFO]\u001b[0m MCP server \"multiply_tool\": Authentication validation passed: 307\n",
      "\u001b[90m[INFO]\u001b[0m MCP server \"multiply_tool\": testing Streamable HTTP support for http://127.0.0.1:8000/mcp\n",
      "\u001b[90m[INFO]\u001b[0m MCP server \"multiply_tool\": detected Streamable HTTP transport support\n",
      "\u001b[90m[INFO]\u001b[0m MCP server \"add_tool\": connected\n",
      "\u001b[90m[INFO]\u001b[0m MCP server \"add_tool\": 1 tool(s) available:\n",
      "\u001b[90m[INFO]\u001b[0m - add\n",
      "\u001b[90m[INFO]\u001b[0m Received session ID: be2c986267d448e188a152e20d0aaf4b\n",
      "\u001b[90m[INFO]\u001b[0m Negotiated protocol version: 2025-06-18\n",
      "\u001b[90m[INFO]\u001b[0m MCP server \"multiply_tool\": connected\n",
      "\u001b[90m[INFO]\u001b[0m MCP server \"multiply_tool\": 1 tool(s) available:\n",
      "\u001b[90m[INFO]\u001b[0m - multiply\n",
      "\u001b[90m[INFO]\u001b[0m MCP servers initialized: 2 tool(s) available in total\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tools: [McpToLangChainAdapter(), McpToLangChainAdapter()]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[90m[INFO]\u001b[0m MCP tool \"add_tool\"/\"add\" received input: {'a': 7, 'b': 13}\n",
      "\u001b[90m[INFO]\u001b[0m MCP tool \"add_tool\"/\"add\" received result (size: 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is 7 + 13?\n",
      "Answer: The answer to the original question, \"What is 7 + 13?\", is 20.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[90m[INFO]\u001b[0m MCP tool \"multiply_tool\"/\"multiply\" received input: {'a': 6, 'b': 9}\n",
      "\u001b[90m[INFO]\u001b[0m MCP tool \"multiply_tool\"/\"multiply\" received result (size: 2)\n",
      "\u001b[90m[INFO]\u001b[0m MCP server \"multiply_tool\": session closed\n",
      "\u001b[90m[INFO]\u001b[0m MCP server \"add_tool\": session closed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Compute 6 × 9.\n",
      "Answer: The result of multiplying 6 and 9 is 54.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
